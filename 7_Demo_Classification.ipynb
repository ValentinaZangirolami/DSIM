{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ycj-iO5sbq3_"
   },
   "source": [
    "**PART 7 - DSIM project**\n",
    "\n",
    "In this section, we test our algorithms to new images with a session of live cam.\n",
    "\n",
    "**Authors:** \n",
    "* Francesca De Cola, matricola 819343  CdLM: Data Science\n",
    "* Valentina Moretto, matricola 853744  CdLM: Data Science\n",
    "* Valentina Zangirolami, matricola 819451  CdLM: Scienze Statistiche ed Economiche (CLAMSES)\n",
    "\n",
    "Summary procedure:\n",
    "1. Start live session and extract frame when it is possible detect face\n",
    "2. Model receive this frame and predict the expression\n",
    "3. Results are visible in live cam with the assignment of label, accuracy and the correspondent emoji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUyld5sxbq4M"
   },
   "source": [
    "**Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cRqilPIObq4P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2 \n",
    "import cv2 as cv\n",
    "from cv2 import VideoCapture as cap\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqLzNVJ8bq4S"
   },
   "source": [
    "Follow lines include two function: **image_resize** and **CFEVideoConf**. In general, this function are imported by packages *utils*, but for some problems we import these function in the following chunck.\n",
    "\n",
    "These function allows to:\n",
    "* image_resize: resize images. In particular we use this function to resize emoji images.\n",
    "* CFEVideoConf, is useful to capture video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-FFiDsEjbq4U"
   },
   "outputs": [],
   "source": [
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "class CFEVideoConf(object):\n",
    "    # Standard Video Dimensions Sizes\n",
    "    STD_DIMENSIONS =  {\n",
    "        \"360p\": (480, 360),\n",
    "        \"480p\": (640, 480),\n",
    "        \"720p\": (1280, 720),\n",
    "        \"1080p\": (1920, 1080),\n",
    "        \"4k\": (3840, 2160),\n",
    "    }\n",
    "    # Video Encoding, might require additional installs\n",
    "    # Types of Codes: http://www.fourcc.org/codecs.php\n",
    "    VIDEO_TYPE = {\n",
    "        'avi': cv2.VideoWriter_fourcc(*'XVID'),\n",
    "        #'mp4': cv2.VideoWriter_fourcc(*'H264'),\n",
    "        'mp4': cv2.VideoWriter_fourcc(*'XVID'),\n",
    "    }\n",
    "\n",
    "    width           = 640\n",
    "    height          = 480\n",
    "    dims            = (640, 480)\n",
    "    capture         = None\n",
    "    video_type      = None\n",
    "    def __init__(self, capture, filepath, res=\"480p\", *args, **kwargs):\n",
    "        self.capture = capture\n",
    "        self.filepath = filepath\n",
    "        self.width, self.height = self.get_dims(res=res)\n",
    "        self.video_type = self.get_video_type()\n",
    "\n",
    "    # Set resolution for the video capture\n",
    "    # Function adapted from https://kirr.co/0l6qmh\n",
    "    def change_res(self, width, height):\n",
    "        self.capture.set(3, width)\n",
    "        self.capture.set(4, height)\n",
    "\n",
    "    def get_dims(self, res='480p'):\n",
    "        width, height = self.STD_DIMENSIONS['480p']\n",
    "        if res in self.STD_DIMENSIONS:\n",
    "            width, height = self.STD_DIMENSIONS[res]\n",
    "        self.change_res(width, height)\n",
    "        self.dims = (width, height)\n",
    "        return width, height\n",
    "\n",
    "    def get_video_type(self):\n",
    "        filename, ext = os.path.splitext(self.filepath)\n",
    "        if ext in self.VIDEO_TYPE:\n",
    "          return  self.VIDEO_TYPE[ext]\n",
    "        return self.VIDEO_TYPE['avi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVZ3n9kYbq4Y"
   },
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej_Cofhibq4a"
   },
   "source": [
    "Initially, we load four models:\n",
    "* model_ft1 and model_ft2: refers to a model in which we apply fine tuning techniques. They represents the two best model of the scripts Fine_tuning_4.ipynb\n",
    "* model_cnn1: refers to our CNN with the best cut of vggface (layer: add_12). It is the last model of the scripts CNN_5\n",
    "* base_model: it is the pretrained neural network VGGFace. It is necessary for model_cnn1, because we use this to extract features of images before model_cnn1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fSB-_1wObq4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\valen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_ft1=load_model('C:/Users/valen/Desktop/MAGISTRALE/DSIM/aml/weights-tmp4.best.hdf5')\n",
    "model_ft2=load_model('C:/Users/valen/Desktop/MAGISTRALE/DSIM/aml/weights-tmp3.best.hdf5')\n",
    "model_cnn1=load_model('C:/Users/valen/Desktop/MAGISTRALE/DSIM/aml_cnn/cnn_best.h5')\n",
    "base_model = VGGFace(include_top = False, input_shape = (224, 224, 3), model='senet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwzxomSRbq4f"
   },
   "source": [
    "We define **label_emoji**: is a function that receives in input images and pre process them with a function of preprocessing of vggface (same preprocessing apply in previous notebook for the consider model). After, we create an ensemble model with all three models in which:\n",
    "1. We extract features of images with vggface, this result is passes to model_cnn1\n",
    "2. For other models we passes directly images such as we have describe in the previous chunck\n",
    "3. After, we define a weighted average of prediction of all models. The value of weights are evaluate in Final_model_6.ipynb.\n",
    "4. Finally, we extract label prediction and define the accuracy and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KP15-ddubq4h"
   },
   "outputs": [],
   "source": [
    "def label_emoji(img):\n",
    "    #preprocessing\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    x = img.astype('float32')\n",
    "    x = preprocess_input(x, version = 2)\n",
    "    \n",
    "    #global variable\n",
    "    global acc\n",
    "    global pred\n",
    "    \n",
    "    #For cnn, images are predict initially with VGGFace (optimal cut: add_12) - feature extraction\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('add_12').output)\n",
    "    predicted = model.predict(x)\n",
    "    #prediction for each model\n",
    "    pred_cnn = model_cnn1.predict(predicted)\n",
    "    pred_ft1 = model_ft1.predict(x)\n",
    "    pred_ft2 = model_ft2.predict(x)\n",
    "    \n",
    "    #ensemble model\n",
    "    final_pred= 0.4*pred_ft1 + 0.1*pred_ft2 + 0.5*pred_cnn \n",
    "    #accuracy\n",
    "    acc = round(np.max(final_pred)*100, 3)\n",
    "    \n",
    "    #predicted labels\n",
    "    pred = np.argmax(final_pred, axis = 1)\n",
    "\n",
    "    return pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYGmNAv5bq4j"
   },
   "source": [
    "We, also, define **detect**, this function receives in input: image extract by video capture, classifier for face detection, counter and rate are parameters useful for video capture, scale_factor and minNeighbors are parameters necessary to face detection and size represents size of images.\n",
    "\n",
    "With this function, we detect face on image of video capture with Cascade Classifier (OpenCV) and define rectangle to print into live cam. Also, we define label and emoji to associate at the expression on video live with the prediction result of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HequkBqhbq4l"
   },
   "outputs": [],
   "source": [
    "def detect(image, classifier, counter, rate,\n",
    "           scale_factor = 1.1, minNeighbors = 5,\n",
    "           size = (224,224)):\n",
    "\n",
    "    face_cascade = cv.CascadeClassifier(classifier)\n",
    "    gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scale_factor, minNeighbors)\n",
    "    \n",
    "    #global variable\n",
    "    global pred\n",
    "    global acc\n",
    "\n",
    "    if len(faces) == 1:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+h] # rec\n",
    "            roi_color = image[y:y+h, x:x+w]\n",
    "            n_img = cv.resize(roi_color, size)\n",
    "        \n",
    "            #color for face, text and rectangle\n",
    "            color_face = (24,191,255) #gold\n",
    "            color_text = (255,255,255) #white\n",
    "            rectangle_bgr = (24,191,255) #gold\n",
    "            \n",
    "            #font e size for text\n",
    "            font = cv.FONT_HERSHEY_TRIPLEX\n",
    "            font_scale = 0.88\n",
    "            \n",
    "            #rectangle for face detection\n",
    "            cv.rectangle(image, (x,y), (x+w,y+h), color_face, 3)\n",
    "            \n",
    "            #extract pred and accuracy to previous function\n",
    "            if counter % rate == 0:\n",
    "                pred, acc = label_emoji(n_img)\n",
    "                \n",
    "            #we define label and emoji to print in live cam\n",
    "            label=''\n",
    "            emoji=''\n",
    "            if pred==0:\n",
    "                label='Angry'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/angry.png\", -1)\n",
    "            elif pred==1:\n",
    "                label='Disgust'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/disgust.png\", -1)\n",
    "            elif pred==2:\n",
    "                label='Fear'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/fear.png\", -1)\n",
    "            elif pred==3:\n",
    "                label='Happy'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/happy.png\", -1)\n",
    "            elif pred==4:\n",
    "                label='Neutral'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/neutral.png\", -1)\n",
    "            elif pred==5:\n",
    "                label='Sad'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/sad.png\", -1)\n",
    "            elif pred==6:\n",
    "                label='Surprise'\n",
    "                emoji = cv.imread(\"C:/Users/valen/Desktop/magistrale/DSIM/emoji/surprised.png\", -1)\n",
    "            else:\n",
    "                label='Unknow'\n",
    "            #string to print\n",
    "            string = \" \" + label + \": \" + str(acc) + \"% \"\n",
    "            \n",
    "            \n",
    "            #Rectangle for background of string\n",
    "            (text_width, text_height) = cv.getTextSize(string, font, fontScale=font_scale, thickness=1)[0]\n",
    "            box_coords = ((x-2, y), (x + text_width + 4, y - text_height - 8))\n",
    "            cv.rectangle(image, box_coords[0], box_coords[1], rectangle_bgr, cv.FILLED)\n",
    "            #insert of string\n",
    "            cv.putText(image, string, (x,y-5), font, font_scale, color_text, 1)\n",
    "            \n",
    "            for (ex, ey, ew, eh) in faces:\n",
    "                roi_face = roi_gray[ey: ey + eh, ex: ex + ew]\n",
    "                emojis = image_resize(emoji.copy(), width = w//4)\n",
    "                \n",
    "                #save shape of face \n",
    "                faces_h, faces_w = faces.shape\n",
    "                \n",
    "                gw, gh, gc = emojis.shape\n",
    "                \n",
    "                for i in range(0, gw):\n",
    "                    for j in range(0, gh):\n",
    "                        #print(emoji[i, j])\n",
    "                        if emojis[i, j][3] != 0: # alpha 0\n",
    "                            offset = 10\n",
    "                            h_offset = faces_h - gh - offset\n",
    "                            w_offset = faces_w - gw - offset\n",
    "                            roi_color[h_offset + i, w_offset + j] = emojis[i, j, :-1]\n",
    "            \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MMVOoY5bq4o"
   },
   "source": [
    "Now, we define **live_cam**. This function is the last function that include all passages that allows to start live session.\n",
    "Initially, we use VideoCapture to read, display and save the video. After, we define a loop that capture a frame, pass it into **detect** function and extrapolate the label, accuracy and emoji and print them on the window session. The terminal of the loop is determinate of the click of the ESC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o8CG1v5tbq4p"
   },
   "outputs": [],
   "source": [
    "def live_cam(classifier, resolution = (1280, 720), fps = 30, time_sleep = 2, rate = 30):\n",
    "  \n",
    "    cam = cv.VideoCapture(0)\n",
    "\n",
    "    cam.set(cv.CAP_PROP_FRAME_WIDTH, resolution[0])\n",
    "    cam.set(cv.CAP_PROP_FRAME_HEIGHT, resolution[1])\n",
    "    cam.set(cv.CAP_PROP_FPS, fps)\n",
    "\n",
    "    cv.namedWindow(\"Acquisition window\")\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        k = cv.waitKey(1)\n",
    "\n",
    "        time.sleep(time_sleep)\n",
    "        detect(frame, classifier=classifier, counter = counter, rate = rate, scale_factor = 1.1, minNeighbors = 6)\n",
    "        \n",
    "        text = 'Press ESC to exit from demo...'\n",
    "        cv.putText(frame, text, (10,20), cv.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 1)\n",
    "        \n",
    "        cv.imshow(\"Acquisition window\", frame)\n",
    "\n",
    "        \n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "    cam.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-TEkgM0bq4r"
   },
   "source": [
    "**LIVE SESSION**\n",
    "\n",
    "We specify the classifier for cascade classifier, we use *haarcascade_frontalface_alt2* that represent a file XML that we load from our folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jbace-qVbq4t",
    "outputId": "158d9aa8-fd3a-4d1b-83a7-cb3a939e6692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "live_cam(time_sleep = 0.01, rate = 10, classifier = 'C:/Users/valen/Desktop/magistrale/DSIM/haarCascade/haarcascade_frontalface_alt2.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_Classification_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
